{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b54ee5",
   "metadata": {},
   "source": [
    "# Predicting the hydrogen storage capacity of alumina pillared interlayer clays using interpretable ensemble machine learning\n",
    "\n",
    "Makungu M. Madirisha a , Lenganji Simwanda b ,  Regina P. Mtei a\n",
    "\n",
    "\n",
    "https://doi.org/10.1016/j.ijhydene.2025.03.216\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fe16a",
   "metadata": {},
   "source": [
    "# Save-only mode + output folder + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c53d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 1: SAVE-ONLY OUTPUT CONFIG\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import os, sys, warnings, contextlib, io, json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# All outputs saved here\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Matplotlib: never display, only save\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # MUST be before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "def savefig(filename: str, fig=None, dpi: int = 300, tight: bool = True):\n",
    "    \"\"\"Save current (or given) figure into OUTPUT_DIR, then close it. No display.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\" if tight else None)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "def save_text(filename: str, text: str):\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(text, encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "def save_json(filename: str, obj):\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(json.dumps(obj, indent=2, default=str), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "# Log file (use log(...) instead of print)\n",
    "LOG_FILE = OUTPUT_DIR / \"run.log\"\n",
    "\n",
    "def log(*args):\n",
    "    msg = \" \".join(str(a) for a in args)\n",
    "    with LOG_FILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "# Optional: capture accidental prints within a block and write to log instead\n",
    "@contextlib.contextmanager\n",
    "def capture_stdout(to_file: Path = LOG_FILE):\n",
    "    old_out, old_err = sys.stdout, sys.stderr\n",
    "    buffer = io.StringIO()\n",
    "    sys.stdout = sys.stderr = buffer\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_out, old_err\n",
    "        content = buffer.getvalue()\n",
    "        if content.strip():\n",
    "            with to_file.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(content + (\"\\n\" if not content.endswith(\"\\n\") else \"\"))\n",
    "\n",
    "log(f\"✅ Save-only mode ON. Outputs folder: {OUTPUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50c9be",
   "metadata": {},
   "source": [
    "# Imports (core + ML + Optuna + SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5f836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 2: IMPORTS\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Optional models (install if needed)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGBRegressor = None\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "\n",
    "import optuna\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "log(\"✅ Imports loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59deb86",
   "metadata": {},
   "source": [
    "# Paths + Load data (Excel) + basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea580390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191 entries, 0 to 190\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   P        191 non-null    float64\n",
      " 1   T        191 non-null    int64  \n",
      " 2   Al/clay  191 non-null    int64  \n",
      " 3   C        191 non-null    float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 6.1 KB\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# CELL 3: LOAD DATA\n",
    "# ==============================\n",
    "DATA_FILE = Path(\"database.xlsx\")      # <-- update if needed\n",
    "SHEET_NAME = 0                      # or \"Sheet1\"\n",
    "TARGET_COL = \"C\"                    # <-- CHANGE this to your target column name\n",
    "\n",
    "assert DATA_FILE.exists(), f\"Missing file: {DATA_FILE.resolve()}\"\n",
    "\n",
    "with capture_stdout():\n",
    "    df = pd.read_excel(DATA_FILE, sheet_name=SHEET_NAME)\n",
    "\n",
    "# Basic numeric coercion (like your notebook style)\n",
    "df = df.copy()\n",
    "for c in df.columns:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "\n",
    "# Drop rows where target missing\n",
    "df = df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "# Save a snapshot of data info\n",
    "save_text(\"data_head.txt\", df.head(20).to_string(index=False))\n",
    "save_text(\"data_info.txt\", str(df.info()))\n",
    "save_json(\"data_shape.json\", {\"rows\": int(df.shape[0]), \"cols\": int(df.shape[1])})\n",
    "\n",
    "log(\"✅ Data loaded:\", df.shape, \"Target:\", TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af00f5",
   "metadata": {},
   "source": [
    "# Split features/target + preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294c9eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('outputs/columns.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# CELL 4: FEATURES / TARGET + PREPROCESS\n",
    "# ==============================\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL].astype(float)\n",
    "\n",
    "# Detect numeric vs categorical (keep simple)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    # (Optional) OneHotEncoder if you have categoricals:\n",
    "    # (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "log(\"✅ Split done. Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "save_json(\"columns.json\", {\"num_cols\": num_cols, \"cat_cols\": cat_cols})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82759b",
   "metadata": {},
   "source": [
    "# EDA plots saved only (pairplot + correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89356f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 5: EDA (SAVE-ONLY)\n",
    "# ==============================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 5A) Correlation heatmap (numeric only)\n",
    "if len(num_cols) >= 2:\n",
    "    corr = df[num_cols + [TARGET_COL]].corr(numeric_only=True)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(corr.values)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.colorbar()\n",
    "    savefig(\"corr_heatmap.png\", fig=fig)\n",
    "    save_text(\"corr_matrix.txt\", corr.to_string())\n",
    "    log(\"✅ Saved corr heatmap + matrix.\")\n",
    "else:\n",
    "    log(\"⚠️ Skipped corr heatmap: not enough numeric columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a0e5a",
   "metadata": {},
   "source": [
    "# Metrics + plotting helpers (save-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210e8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 6: METRICS + PLOT HELPERS\n",
    "# ==============================\n",
    "def mape(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100.0\n",
    "\n",
    "def evaluate_regression(name, y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"mape_percent\": float(mape(y_true, y_pred)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title, filename):\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=10)\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(title)\n",
    "    # 45-degree line\n",
    "    mn = min(np.min(y_true), np.min(y_pred))\n",
    "    mx = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([mn, mx], [mn, mx])\n",
    "    savefig(filename, fig=fig)\n",
    "    return filename\n",
    "\n",
    "log(\"✅ Helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5964c8f",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54f8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 7: MODELS\n",
    "# ==============================\n",
    "models = {}\n",
    "\n",
    "# Baselines\n",
    "models[\"DecisionTree\"] = DecisionTreeRegressor(random_state=42)\n",
    "models[\"AdaBoost\"] = AdaBoostRegressor(random_state=42)\n",
    "models[\"GradientBoosting\"] = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Optional advanced\n",
    "if XGBRegressor is not None:\n",
    "    models[\"XGBoost\"] = XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0\n",
    "    )\n",
    "else:\n",
    "    log(\"⚠️ XGBoost not available (xgboost not installed).\")\n",
    "\n",
    "if CatBoostRegressor is not None:\n",
    "    models[\"CatBoost\"] = CatBoostRegressor(\n",
    "        random_state=42,\n",
    "        verbose=False,\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        depth=6\n",
    "    )\n",
    "else:\n",
    "    log(\"⚠️ CatBoost not available (catboost not installed).\")\n",
    "\n",
    "log(\"✅ Models defined:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65d2bb",
   "metadata": {},
   "source": [
    "# Train + evaluate all models (save metrics + plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c175bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 8: TRAIN + EVALUATE (SAVE-ONLY)\n",
    "# ==============================\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "\n",
    "results = []\n",
    "artifacts = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", clone(model))])\n",
    "\n",
    "    with capture_stdout():\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "    yhat_train = pipe.predict(X_train)\n",
    "    yhat_test = pipe.predict(X_test)\n",
    "\n",
    "    train_metrics = evaluate_regression(name + \"_train\", y_train, yhat_train)\n",
    "    test_metrics = evaluate_regression(name + \"_test\", y_test, yhat_test)\n",
    "    results.extend([train_metrics, test_metrics])\n",
    "\n",
    "    # Save prediction plots\n",
    "    plot_predictions(y_test, yhat_test, f\"{name} (Test)\", f\"pred_{name}_test.png\")\n",
    "\n",
    "    # Save model artifact\n",
    "    model_path = OUTPUT_DIR / f\"{name}_pipeline.joblib\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "    artifacts[name] = str(model_path)\n",
    "\n",
    "    log(f\"✅ Finished {name}: R2(test)={test_metrics['r2']:.4f}\")\n",
    "\n",
    "# Save results table\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv = OUTPUT_DIR / \"metrics_all_models.csv\"\n",
    "results_df.to_csv(results_csv, index=False)\n",
    "save_text(\"metrics_all_models.txt\", results_df.to_string(index=False))\n",
    "save_json(\"model_artifacts.json\", artifacts)\n",
    "\n",
    "log(\"✅ Saved metrics + plots + models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34858b",
   "metadata": {},
   "source": [
    "# Optuna tuning (example: tune XGBoost or CatBoost) + save best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42fa23e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-01 18:50:20,733] A new study created in memory with name: no-name-c06b72e7-abe6-4124-88e1-cb0b27a9bff7\n",
      "[I 2026-01-01 18:50:21,718] Trial 0 finished with value: 0.9816252177140493 and parameters: {'n_estimators': 752, 'learning_rate': 0.12395218232602771, 'max_depth': 3, 'subsample': 0.5234281906293143, 'colsample_bytree': 0.8666481000274028, 'reg_lambda': 1.2128040709835604}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:23,301] Trial 1 finished with value: 0.9708655882571874 and parameters: {'n_estimators': 957, 'learning_rate': 0.013987054923686942, 'max_depth': 4, 'subsample': 0.6277512406888373, 'colsample_bytree': 0.8882355404587481, 'reg_lambda': 0.04917766986868437}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:25,324] Trial 2 finished with value: 0.9687924614610628 and parameters: {'n_estimators': 1845, 'learning_rate': 0.13855354131417139, 'max_depth': 4, 'subsample': 0.5183422894768559, 'colsample_bytree': 0.8699500642311104, 'reg_lambda': 0.03605467771712676}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:26,504] Trial 3 finished with value: 0.8634674489345764 and parameters: {'n_estimators': 913, 'learning_rate': 0.06643912421753395, 'max_depth': 10, 'subsample': 0.9902307450162202, 'colsample_bytree': 0.8898924720260212, 'reg_lambda': 0.009432300135468459}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:28,943] Trial 4 finished with value: 0.9321120998227326 and parameters: {'n_estimators': 1418, 'learning_rate': 0.029166464599827576, 'max_depth': 10, 'subsample': 0.6034262890995421, 'colsample_bytree': 0.6693425958331589, 'reg_lambda': 0.22140495832663157}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:31,057] Trial 5 finished with value: 0.8799832311312425 and parameters: {'n_estimators': 1715, 'learning_rate': 0.03338084548142959, 'max_depth': 9, 'subsample': 0.9221595094660319, 'colsample_bytree': 0.917101943782564, 'reg_lambda': 0.011569381215870748}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:33,309] Trial 6 finished with value: 0.9725680160665064 and parameters: {'n_estimators': 1331, 'learning_rate': 0.047691962472007805, 'max_depth': 4, 'subsample': 0.8123604596319365, 'colsample_bytree': 0.9012106834313471, 'reg_lambda': 0.551412078123076}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:35,208] Trial 7 finished with value: 0.9469218068521004 and parameters: {'n_estimators': 678, 'learning_rate': 0.010872149033998258, 'max_depth': 7, 'subsample': 0.8135498522250882, 'colsample_bytree': 0.7457423372244241, 'reg_lambda': 1.049222836288909}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:37,504] Trial 8 finished with value: 0.9260188813898977 and parameters: {'n_estimators': 1730, 'learning_rate': 0.03821638470745624, 'max_depth': 6, 'subsample': 0.7644292103009587, 'colsample_bytree': 0.9447729905233136, 'reg_lambda': 0.03049185905041566}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:38,192] Trial 9 finished with value: 0.9350774818728244 and parameters: {'n_estimators': 312, 'learning_rate': 0.10773020112302054, 'max_depth': 6, 'subsample': 0.5666668808437015, 'colsample_bytree': 0.9875237853430274, 'reg_lambda': 0.0016892511323034886}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:38,618] Trial 10 finished with value: 0.867381239555091 and parameters: {'n_estimators': 255, 'learning_rate': 0.1787797487054576, 'max_depth': 2, 'subsample': 0.6773844911662297, 'colsample_bytree': 0.52884449501693, 'reg_lambda': 8.552085288540537}. Best is trial 0 with value: 0.9816252177140493.\n",
      "[I 2026-01-01 18:50:40,291] Trial 11 finished with value: 0.983302658795892 and parameters: {'n_estimators': 1321, 'learning_rate': 0.07615993098392637, 'max_depth': 2, 'subsample': 0.8376163136284034, 'colsample_bytree': 0.7812786824899361, 'reg_lambda': 0.9581408548306404}. Best is trial 11 with value: 0.983302658795892.\n",
      "[I 2026-01-01 18:50:41,201] Trial 12 finished with value: 0.9832847898720682 and parameters: {'n_estimators': 654, 'learning_rate': 0.08812347985797621, 'max_depth': 2, 'subsample': 0.8926707364458458, 'colsample_bytree': 0.784708224432081, 'reg_lambda': 4.84593647939228}. Best is trial 11 with value: 0.983302658795892.\n",
      "[I 2026-01-01 18:50:43,021] Trial 13 finished with value: 0.9842256905550159 and parameters: {'n_estimators': 1263, 'learning_rate': 0.07348217689377916, 'max_depth': 2, 'subsample': 0.8912191901868337, 'colsample_bytree': 0.757312127616775, 'reg_lambda': 6.429924079241321}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:44,952] Trial 14 finished with value: 0.9841389936324625 and parameters: {'n_estimators': 1296, 'learning_rate': 0.05727711974197097, 'max_depth': 2, 'subsample': 0.8649271733715592, 'colsample_bytree': 0.6740852754785082, 'reg_lambda': 2.5843797921835447}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:47,145] Trial 15 finished with value: 0.8444263985423024 and parameters: {'n_estimators': 1170, 'learning_rate': 0.02426951230035312, 'max_depth': 5, 'subsample': 0.9648259374466877, 'colsample_bytree': 0.6297356493865727, 'reg_lambda': 3.0869681792969086}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:49,534] Trial 16 finished with value: 0.9811769310658081 and parameters: {'n_estimators': 1527, 'learning_rate': 0.05396132237659395, 'max_depth': 3, 'subsample': 0.8869548753639915, 'colsample_bytree': 0.6700006017341055, 'reg_lambda': 0.24660083803360297}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:51,362] Trial 17 finished with value: 0.8455427871173887 and parameters: {'n_estimators': 1071, 'learning_rate': 0.02081292005454162, 'max_depth': 8, 'subsample': 0.7135047209211303, 'colsample_bytree': 0.5568419970768623, 'reg_lambda': 2.8479653891443903}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:54,053] Trial 18 finished with value: 0.9829890793661331 and parameters: {'n_estimators': 1980, 'learning_rate': 0.06701158461104957, 'max_depth': 3, 'subsample': 0.8597090131610783, 'colsample_bytree': 0.7320039381854903, 'reg_lambda': 5.248812787526565}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:55,866] Trial 19 finished with value: 0.7908312941735144 and parameters: {'n_estimators': 1597, 'learning_rate': 0.1909266833474704, 'max_depth': 5, 'subsample': 0.765144605997186, 'colsample_bytree': 0.5965605929292674, 'reg_lambda': 0.29694511388469813}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:57,716] Trial 20 finished with value: 0.9826329411612523 and parameters: {'n_estimators': 1180, 'learning_rate': 0.04826366361299057, 'max_depth': 2, 'subsample': 0.9270965205137822, 'colsample_bytree': 0.704030112111917, 'reg_lambda': 2.0042792586690923}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:50:59,544] Trial 21 finished with value: 0.9831683310215652 and parameters: {'n_estimators': 1337, 'learning_rate': 0.08793465090288362, 'max_depth': 2, 'subsample': 0.8286871073826294, 'colsample_bytree': 0.8031366326072763, 'reg_lambda': 0.9732503883140462}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:51:01,769] Trial 22 finished with value: 0.9835749749539285 and parameters: {'n_estimators': 1294, 'learning_rate': 0.07045321803598148, 'max_depth': 3, 'subsample': 0.86053839924776, 'colsample_bytree': 0.8173275921622184, 'reg_lambda': 9.95539795991328}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:51:03,955] Trial 23 finished with value: 0.9830782430948004 and parameters: {'n_estimators': 1493, 'learning_rate': 0.05804069042961569, 'max_depth': 3, 'subsample': 0.9523686519295329, 'colsample_bytree': 0.8296189484635011, 'reg_lambda': 8.05282234895528}. Best is trial 13 with value: 0.9842256905550159.\n",
      "[I 2026-01-01 18:51:05,775] Trial 24 finished with value: 0.984749437278516 and parameters: {'n_estimators': 1044, 'learning_rate': 0.09798516712925766, 'max_depth': 3, 'subsample': 0.8847154107776427, 'colsample_bytree': 0.7010451800303613, 'reg_lambda': 9.946790900427247}. Best is trial 24 with value: 0.984749437278516.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-01 18:51:07,547] Trial 25 finished with value: 0.9707912816429303 and parameters: {'n_estimators': 1001, 'learning_rate': 0.103199788616281, 'max_depth': 5, 'subsample': 0.9043439994752943, 'colsample_bytree': 0.6732222310306423, 'reg_lambda': 2.8806754217068673}. Best is trial 24 with value: 0.984749437278516.\n",
      "[I 2026-01-01 18:51:08,815] Trial 26 finished with value: 0.8191105223522935 and parameters: {'n_estimators': 890, 'learning_rate': 0.15014712251821655, 'max_depth': 4, 'subsample': 0.9957013010840189, 'colsample_bytree': 0.6083368058400126, 'reg_lambda': 4.467923461730908}. Best is trial 24 with value: 0.984749437278516.\n",
      "[I 2026-01-01 18:51:09,756] Trial 27 finished with value: 0.9781521425507673 and parameters: {'n_estimators': 517, 'learning_rate': 0.040274265831966506, 'max_depth': 2, 'subsample': 0.7965433434992114, 'colsample_bytree': 0.7164094816337236, 'reg_lambda': 2.017097137902855}. Best is trial 24 with value: 0.984749437278516.\n",
      "[I 2026-01-01 18:51:11,716] Trial 28 finished with value: 0.8113811037999262 and parameters: {'n_estimators': 1182, 'learning_rate': 0.09091414204866038, 'max_depth': 3, 'subsample': 0.8708607994379851, 'colsample_bytree': 0.6296058769458853, 'reg_lambda': 0.41546209930878625}. Best is trial 24 with value: 0.984749437278516.\n",
      "[I 2026-01-01 18:51:13,013] Trial 29 finished with value: 0.9631259279899341 and parameters: {'n_estimators': 845, 'learning_rate': 0.1322726866316954, 'max_depth': 4, 'subsample': 0.7191517323766122, 'colsample_bytree': 0.750536036507606, 'reg_lambda': 0.1028832053391248}. Best is trial 24 with value: 0.984749437278516.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# CELL 9: OPTUNA TUNING (OPTIONAL)\n",
    "# ==============================\n",
    "BEST_MODEL_NAME = None\n",
    "BEST_PIPE = None\n",
    "BEST_SCORE = -np.inf\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    if XGBRegressor is None:\n",
    "        return 0.0\n",
    "\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "\n",
    "    # CV score\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in kf.split(X_train):\n",
    "        Xtr, Xva = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "        ytr, yva = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
    "        with capture_stdout():\n",
    "            pipe.fit(Xtr, ytr)\n",
    "        pred = pipe.predict(Xva)\n",
    "        scores.append(r2_score(yva, pred))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "if XGBRegressor is not None:\n",
    "    with capture_stdout():\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective_xgb, n_trials=30)\n",
    "\n",
    "    save_json(\"optuna_best_xgb.json\", study.best_params)\n",
    "    log(\"✅ Optuna best XGB params:\", study.best_params)\n",
    "\n",
    "    # Train best model on full train\n",
    "    best_model = XGBRegressor(**study.best_params, random_state=42)\n",
    "    BEST_PIPE = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", best_model)])\n",
    "    with capture_stdout():\n",
    "        BEST_PIPE.fit(X_train, y_train)\n",
    "\n",
    "    yhat = BEST_PIPE.predict(X_test)\n",
    "    best_metrics = evaluate_regression(\"XGB_Optuna_test\", y_test, yhat)\n",
    "    save_json(\"best_model_metrics.json\", best_metrics)\n",
    "    plot_predictions(y_test, yhat, \"XGB Optuna (Test)\", \"pred_best_xgb_optuna.png\")\n",
    "\n",
    "    BEST_MODEL_NAME = \"XGB_Optuna\"\n",
    "    BEST_SCORE = best_metrics[\"r2\"]\n",
    "\n",
    "    import joblib\n",
    "    joblib.dump(BEST_PIPE, OUTPUT_DIR / \"BEST_pipeline.joblib\")\n",
    "else:\n",
    "    log(\"⚠️ Skipped Optuna: XGBoost not installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cccd4b",
   "metadata": {},
   "source": [
    "# SHAP explainability (save-only, no display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1c4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 10: SHAP (SAVE-ONLY)\n",
    "# ==============================\n",
    "if shap is None:\n",
    "    log(\"⚠️ SHAP not available (shap not installed). Skipping.\")\n",
    "else:\n",
    "    # Choose pipeline: best if available, else pick one model artifact\n",
    "    import joblib\n",
    "    if (OUTPUT_DIR / \"BEST_pipeline.joblib\").exists():\n",
    "        pipe = joblib.load(OUTPUT_DIR / \"BEST_pipeline.joblib\")\n",
    "        model_name = \"BEST\"\n",
    "    else:\n",
    "        # fallback: use CatBoost if exists, else first saved model\n",
    "        first = next(iter(models.keys()))\n",
    "        pipe = joblib.load(OUTPUT_DIR / f\"{first}_pipeline.joblib\")\n",
    "        model_name = first\n",
    "\n",
    "    # Transform X_test to model input space (after preprocessing)\n",
    "    X_test_proc = pipe.named_steps[\"preprocess\"].transform(X_test)\n",
    "    model = pipe.named_steps[\"model\"]\n",
    "\n",
    "    # SHAP explainer (tree models typically work with TreeExplainer)\n",
    "    with capture_stdout():\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test_proc)\n",
    "        except Exception:\n",
    "            explainer = shap.Explainer(model, X_test_proc)\n",
    "            shap_values = explainer(X_test_proc)\n",
    "\n",
    "    # Summary plot saved\n",
    "    fig = plt.figure()\n",
    "    with capture_stdout():\n",
    "        try:\n",
    "            shap.summary_plot(shap_values, X_test_proc, show=False)\n",
    "        except Exception:\n",
    "            # Some shap versions use shap_values.values\n",
    "            shap.summary_plot(getattr(shap_values, \"values\", shap_values), X_test_proc, show=False)\n",
    "\n",
    "    savefig(f\"shap_summary_{model_name}.png\", fig=fig)\n",
    "    log(f\"✅ Saved SHAP summary: shap_summary_{model_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff1173",
   "metadata": {},
   "source": [
    "# Final “run report” saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13fb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 11: FINAL REPORT\n",
    "# ==============================\n",
    "report_lines = []\n",
    "report_lines.append(\"RUN COMPLETE\\n\")\n",
    "report_lines.append(f\"Data file: {DATA_FILE}\\n\")\n",
    "report_lines.append(f\"Rows/Cols: {df.shape}\\n\")\n",
    "report_lines.append(f\"Target: {TARGET_COL}\\n\")\n",
    "report_lines.append(f\"Numeric cols: {len(num_cols)} | Categorical cols: {len(cat_cols)}\\n\")\n",
    "\n",
    "metrics_path = OUTPUT_DIR / \"metrics_all_models.csv\"\n",
    "if metrics_path.exists():\n",
    "    report_lines.append(f\"Metrics: {metrics_path.name}\\n\")\n",
    "\n",
    "if (OUTPUT_DIR / \"BEST_pipeline.joblib\").exists():\n",
    "    report_lines.append(\"Best model saved: BEST_pipeline.joblib\\n\")\n",
    "    if (OUTPUT_DIR / \"best_model_metrics.json\").exists():\n",
    "        report_lines.append(\"Best model metrics: best_model_metrics.json\\n\")\n",
    "\n",
    "save_text(\"RUN_REPORT.txt\", \"\".join(report_lines))\n",
    "log(\"✅ Saved RUN_REPORT.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
