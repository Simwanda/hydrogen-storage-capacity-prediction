{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b54ee5",
   "metadata": {},
   "source": [
    "# Predicting the hydrogen storage capacity of alumina pillared interlayer clays using interpretable ensemble machine learning\n",
    "\n",
    "Makungu M. Madirisha a , Lenganji Simwanda b ,  Regina P. Mtei a\n",
    "\n",
    "\n",
    "https://doi.org/10.1016/j.ijhydene.2025.03.216\n",
    "\n",
    "CODE BY LENGANJI SIMWANDA, COPYRIGHT © 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fe16a",
   "metadata": {},
   "source": [
    "# Save-only mode + output folder + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c53d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 1: SAVE-ONLY OUTPUT CONFIG\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import os, sys, warnings, contextlib, io, json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# All outputs saved here\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Matplotlib: never display, only save\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # MUST be before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "def savefig(filename: str, fig=None, dpi: int = 300, tight: bool = True):\n",
    "    \"\"\"Save current (or given) figure into OUTPUT_DIR, then close it. No display.\"\"\"\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\" if tight else None)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "def save_text(filename: str, text: str):\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(text, encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "def save_json(filename: str, obj):\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(json.dumps(obj, indent=2, default=str), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "# Log file (use log(...) instead of print)\n",
    "LOG_FILE = OUTPUT_DIR / \"run.log\"\n",
    "\n",
    "def log(*args):\n",
    "    msg = \" \".join(str(a) for a in args)\n",
    "    with LOG_FILE.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg + \"\\n\")\n",
    "\n",
    "# Optional: capture accidental prints within a block and write to log instead\n",
    "@contextlib.contextmanager\n",
    "def capture_stdout(to_file: Path = LOG_FILE):\n",
    "    old_out, old_err = sys.stdout, sys.stderr\n",
    "    buffer = io.StringIO()\n",
    "    sys.stdout = sys.stderr = buffer\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = old_out, old_err\n",
    "        content = buffer.getvalue()\n",
    "        if content.strip():\n",
    "            with to_file.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(content + (\"\\n\" if not content.endswith(\"\\n\") else \"\"))\n",
    "\n",
    "log(f\"✅ Save-only mode ON. Outputs folder: {OUTPUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50c9be",
   "metadata": {},
   "source": [
    "# Imports (core + ML + Optuna + SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5f836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 2: IMPORTS\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Optional models (install if needed)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGBRegressor = None\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "\n",
    "import optuna\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "log(\"✅ Imports loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59deb86",
   "metadata": {},
   "source": [
    "# Paths + Load data (Excel) + basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea580390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 191 entries, 0 to 190\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   P        191 non-null    float64\n",
      " 1   T        191 non-null    int64  \n",
      " 2   Al/clay  191 non-null    int64  \n",
      " 3   C        191 non-null    float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 6.1 KB\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# CELL 3: LOAD DATA\n",
    "# ==============================\n",
    "DATA_FILE = Path(\"database.xlsx\")      # <-- update if needed\n",
    "SHEET_NAME = 0                      # or \"Sheet1\"\n",
    "TARGET_COL = \"C\"                    # <-- CHANGE this to your target column name\n",
    "\n",
    "assert DATA_FILE.exists(), f\"Missing file: {DATA_FILE.resolve()}\"\n",
    "\n",
    "with capture_stdout():\n",
    "    df = pd.read_excel(DATA_FILE, sheet_name=SHEET_NAME)\n",
    "\n",
    "# Basic numeric coercion (like your notebook style)\n",
    "df = df.copy()\n",
    "for c in df.columns:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "\n",
    "# Drop rows where target missing\n",
    "df = df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "# Save a snapshot of data info\n",
    "save_text(\"data_head.txt\", df.head(20).to_string(index=False))\n",
    "save_text(\"data_info.txt\", str(df.info()))\n",
    "save_json(\"data_shape.json\", {\"rows\": int(df.shape[0]), \"cols\": int(df.shape[1])})\n",
    "\n",
    "log(\"✅ Data loaded:\", df.shape, \"Target:\", TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af00f5",
   "metadata": {},
   "source": [
    "# Split features/target + preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294c9eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('outputs/columns.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# CELL 4: FEATURES / TARGET + PREPROCESS\n",
    "# ==============================\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL].astype(float)\n",
    "\n",
    "# Detect numeric vs categorical (keep simple)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    # (Optional) OneHotEncoder if you have categoricals:\n",
    "    # (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "log(\"✅ Split done. Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "save_json(\"columns.json\", {\"num_cols\": num_cols, \"cat_cols\": cat_cols})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82759b",
   "metadata": {},
   "source": [
    "# EDA plots saved only (pairplot + correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89356f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 5: EDA (SAVE-ONLY)\n",
    "# ==============================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 5A) Correlation heatmap (numeric only)\n",
    "if len(num_cols) >= 2:\n",
    "    corr = df[num_cols + [TARGET_COL]].corr(numeric_only=True)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(corr.values)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.colorbar()\n",
    "    savefig(\"corr_heatmap.png\", fig=fig)\n",
    "    save_text(\"corr_matrix.txt\", corr.to_string())\n",
    "    log(\"✅ Saved corr heatmap + matrix.\")\n",
    "else:\n",
    "    log(\"⚠️ Skipped corr heatmap: not enough numeric columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a0e5a",
   "metadata": {},
   "source": [
    "# Metrics + plotting helpers (save-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210e8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 6: METRICS + PLOT HELPERS (SAVE-ONLY)\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100.0\n",
    "\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "    return {\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"mape\": float(mape(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def parity_plot(ax, y_train, y_train_pred, y_test, y_test_pred, title,\n",
    "                xlabel=r\"$H_{2,exp}$ (mmmol STP/g)\", ylabel=r\"$H_{2,pred}$ (mmmol STP/g)\"):\n",
    "    # Training: blue circles\n",
    "    ax.scatter(y_train, y_train_pred, c=\"blue\", s=18, marker=\"o\", label=\"Training set\")\n",
    "    # Testing: red squares (matches your sample better than triangles)\n",
    "    ax.scatter(y_test, y_test_pred, c=\"red\", s=22, marker=\"s\", label=\"Testing set\")\n",
    "\n",
    "    # Perfect prediction line using combined min/max\n",
    "    mn = float(min(np.min(y_train), np.min(y_test), np.min(y_train_pred), np.min(y_test_pred)))\n",
    "    mx = float(max(np.max(y_train), np.max(y_test), np.max(y_train_pred), np.max(y_test_pred)))\n",
    "    ax.plot([mn, mx], [mn, mx], color=\"gray\", linewidth=2, label=\"Perfect Prediction\")\n",
    "\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "\n",
    "    ax.legend(loc=\"upper left\", frameon=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Optional: start axes at 0 if your data is non-negative\n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "def plot_metrics_2x2(metrics_dict, filename=\"metrics_2x2.png\"):\n",
    "    \"\"\"\n",
    "    metrics_dict example:\n",
    "      {\n",
    "        \"AdaBoost\": {\"train\": {...}, \"test\": {...}},\n",
    "        \"GBM\": {...}, \"CatBoost\": {...}, \"XGBoost\": {...}\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Keep order like your figure\n",
    "    order = [\"XGBoost\", \"CatBoost\", \"GBM\", \"AdaBoost\"]\n",
    "    order = [m for m in order if m in metrics_dict]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    keys = [(\"r2\", r\"$(a)\\ r^2$\"),\n",
    "            (\"rmse\", \"(b) RMSE\"),\n",
    "            (\"mae\", \"(c) MAE\"),\n",
    "            (\"mape\", \"(d) MAPE(%)\")]\n",
    "\n",
    "    for ax, (k, title) in zip(axes, keys):\n",
    "        y_pos = np.arange(len(order))\n",
    "\n",
    "        train_vals = [metrics_dict[m][\"train\"][k] for m in order]\n",
    "        test_vals  = [metrics_dict[m][\"test\"][k]  for m in order]\n",
    "\n",
    "        # Horizontal bars (training blue, testing red) like your sample\n",
    "        ax.barh(y_pos - 0.18, train_vals, height=0.35, label=\"Training set\")\n",
    "        ax.barh(y_pos + 0.18, test_vals,  height=0.35, label=\"Testing set\")\n",
    "\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(order)\n",
    "        ax.set_title(title)\n",
    "\n",
    "        # Annotate values on bars\n",
    "        for i, (tv, sv) in enumerate(zip(train_vals, test_vals)):\n",
    "            ax.text(tv, i - 0.18, f\" {tv:.3f}\", va=\"center\", fontsize=9)\n",
    "            ax.text(sv, i + 0.18, f\" {sv:.3f}\", va=\"center\", fontsize=9)\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "    axes[0].legend(loc=\"upper right\", frameon=False)\n",
    "    savefig(filename, fig=fig)\n",
    "    log(f\"✅ Saved metrics figure: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5964c8f",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54f8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 7: MODELS\n",
    "# ==============================\n",
    "models = {}\n",
    "\n",
    "# Baselines\n",
    "models[\"DecisionTree\"] = DecisionTreeRegressor(random_state=42)\n",
    "models[\"AdaBoost\"] = AdaBoostRegressor(random_state=42)\n",
    "models[\"GradientBoosting\"] = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Optional advanced\n",
    "if XGBRegressor is not None:\n",
    "    models[\"XGBoost\"] = XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0\n",
    "    )\n",
    "else:\n",
    "    log(\"⚠️ XGBoost not available (xgboost not installed).\")\n",
    "\n",
    "if CatBoostRegressor is not None:\n",
    "    models[\"CatBoost\"] = CatBoostRegressor(\n",
    "        random_state=42,\n",
    "        verbose=False,\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        depth=6\n",
    "    )\n",
    "else:\n",
    "    log(\"⚠️ CatBoost not available (catboost not installed).\")\n",
    "\n",
    "log(\"✅ Models defined:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65d2bb",
   "metadata": {},
   "source": [
    "# Train + evaluate all models (save metrics + plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1c175bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 8: TRAIN + EVALUATE + PARITY(2x2) + METRICS(2x2) (SAVE-ONLY)\n",
    "# ==============================\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# ------------------------------\n",
    "# Silence Optuna & warnings\n",
    "# ------------------------------\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------\n",
    "# Containers\n",
    "# ------------------------------\n",
    "results_rows = []\n",
    "pred_store = {}\n",
    "metrics_store = {}\n",
    "artifacts = {}\n",
    "\n",
    "# ------------------------------\n",
    "# Optuna setup\n",
    "# ------------------------------\n",
    "TUNING_DIR = OUTPUT_DIR / \"optuna\"\n",
    "TUNING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_TRIALS = 60\n",
    "CV_SPLITS = 5\n",
    "\n",
    "cv = KFold(n_splits=CV_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# ------------------------------\n",
    "# CV RMSE (version-safe)\n",
    "# ------------------------------\n",
    "def _cv_rmse(model):\n",
    "    rmses = []\n",
    "    for tr_idx, va_idx in cv.split(X_train):\n",
    "        Xtr, Xva = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "        ytr, yva = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        pred = pipe.predict(Xva)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(yva, pred))\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return float(np.mean(rmses))\n",
    "\n",
    "# ------------------------------\n",
    "# Optuna tuning function\n",
    "# ------------------------------\n",
    "def tune_model(model_name):\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    pruner = optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        if model_name == \"AdaBoost\":\n",
    "            max_depth = trial.suggest_int(\"base_estimator_max_depth\", 1, 10)\n",
    "            base_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "\n",
    "            params = dict(\n",
    "                n_estimators=trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "                learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1.0, log=True),\n",
    "                loss=trial.suggest_categorical(\"loss\", [\"linear\", \"square\", \"exponential\"]),\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                model = AdaBoostRegressor(estimator=base_tree, **params)\n",
    "            except TypeError:\n",
    "                model = AdaBoostRegressor(base_estimator=base_tree, **params)\n",
    "\n",
    "        elif model_name == \"GradientBoosting\":\n",
    "            model = GradientBoostingRegressor(\n",
    "                n_estimators=trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.5, log=True),\n",
    "                max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "                max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "                subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        elif model_name == \"XGBoost\":\n",
    "            if XGBRegressor is None:\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            model = XGBRegressor(\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_estimators=trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                subsample=trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "                reg_lambda=trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "        elif model_name == \"CatBoost\":\n",
    "            if CatBoostRegressor is None:\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=trial.suggest_int(\"iterations\", 100, 1000),\n",
    "                depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "                learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                random_strength=trial.suggest_int(\"random_strength\", 0, 100),\n",
    "                bagging_temperature=trial.suggest_float(\"bagging_temperature\", 0.01, 1.0, log=True),\n",
    "                l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True),\n",
    "                border_count=trial.suggest_int(\"border_count\", 50, 255),\n",
    "                loss_function=\"RMSE\",\n",
    "                verbose=False,\n",
    "                random_seed=42\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return _cv_rmse(models[model_name])\n",
    "\n",
    "        return _cv_rmse(model)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=sampler,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best_params = dict(study.best_params)\n",
    "\n",
    "    # --------------------------\n",
    "    # Rebuild best model\n",
    "    # --------------------------\n",
    "    if model_name == \"AdaBoost\":\n",
    "        md = best_params.pop(\"base_estimator_max_depth\")\n",
    "        base_tree = DecisionTreeRegressor(max_depth=md, random_state=42)\n",
    "        try:\n",
    "            best_model = AdaBoostRegressor(estimator=base_tree, random_state=42, **best_params)\n",
    "        except TypeError:\n",
    "            best_model = AdaBoostRegressor(base_estimator=base_tree, random_state=42, **best_params)\n",
    "\n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        best_model = GradientBoostingRegressor(random_state=42, **best_params)\n",
    "\n",
    "    elif model_name == \"XGBoost\":\n",
    "        best_model = XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            **best_params\n",
    "        )\n",
    "\n",
    "    elif model_name == \"CatBoost\":\n",
    "        best_model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            verbose=False,\n",
    "            random_seed=42,\n",
    "            **best_params\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        best_model = models[model_name]\n",
    "\n",
    "    # Save best params only (no history)\n",
    "    with open(TUNING_DIR / f\"{model_name}_best_params.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"best_cv_rmse\": float(study.best_value),\n",
    "                \"best_params\": best_params\n",
    "            },\n",
    "            f,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "    log(f\"✅ Optuna tuned {model_name}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# ------------------------------\n",
    "# Tune selected models\n",
    "# ------------------------------\n",
    "to_tune = [\"AdaBoost\", \"GradientBoosting\", \"XGBoost\", \"CatBoost\"]\n",
    "to_tune = [m for m in to_tune if m in models]\n",
    "\n",
    "for mname in to_tune:\n",
    "    try:\n",
    "        models[mname] = tune_model(mname)\n",
    "    except Exception as e:\n",
    "        log(f\"⚠️ Tuning skipped for {mname}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Train & evaluate models\n",
    "# ------------------------------\n",
    "for name, model in models.items():\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", clone(model))\n",
    "    ])\n",
    "\n",
    "    with capture_stdout():\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "    ytr = pipe.predict(X_train)\n",
    "    yte = pipe.predict(X_test)\n",
    "\n",
    "    m_tr = evaluate_regression(y_train, ytr)\n",
    "    m_te = evaluate_regression(y_test, yte)\n",
    "\n",
    "    pred_store[name] = {\"ytr\": ytr, \"yte\": yte}\n",
    "    metrics_store[name] = {\"train\": m_tr, \"test\": m_te}\n",
    "\n",
    "    results_rows.append({\"model\": name, \"split\": \"train\", **m_tr})\n",
    "    results_rows.append({\"model\": name, \"split\": \"test\", **m_te})\n",
    "\n",
    "    model_path = OUTPUT_DIR / f\"{name}_pipeline.joblib\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "    artifacts[name] = str(model_path)\n",
    "\n",
    "    log(f\"✅ {name} done | R2(test)={m_te['r2']:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Save metrics\n",
    "# ------------------------------\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "results_df.to_csv(OUTPUT_DIR / \"metrics_all_models.csv\", index=False)\n",
    "save_text(\"metrics_all_models.txt\", results_df.to_string(index=False))\n",
    "save_json(\"model_artifacts.json\", artifacts)\n",
    "\n",
    "# ------------------------------\n",
    "# Parity plots (2x2)\n",
    "# ------------------------------\n",
    "parity_order = [\"AdaBoost\", \"GradientBoosting\", \"CatBoost\", \"XGBoost\"]\n",
    "titles = {\n",
    "    \"AdaBoost\": \"(a) AdaBoost\",\n",
    "    \"GradientBoosting\": \"(b) GBM\",\n",
    "    \"CatBoost\": \"(c) CatBoost\",\n",
    "    \"XGBoost\": \"(d) XGBoost\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, mname in zip(axes, parity_order):\n",
    "    if mname not in pred_store:\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    parity_plot(\n",
    "        ax=ax,\n",
    "        y_train=y_train,\n",
    "        y_train_pred=pred_store[mname][\"ytr\"],\n",
    "        y_test=y_test,\n",
    "        y_test_pred=pred_store[mname][\"yte\"],\n",
    "        title=titles[mname],\n",
    "        xlabel=r\"$H_{2,exp}$ (mmmol STP/g)\",\n",
    "        ylabel=r\"$H_{2,pred}$ (mmmol STP/g)\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"parity_2x2.png\", fig=fig)\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics plots\n",
    "# ------------------------------\n",
    "plot_metrics_2x2(metrics_store, filename=\"metrics_2x2.png\")\n",
    "\n",
    "log(\"✅ All models tuned, trained, evaluated, and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cccd4b",
   "metadata": {},
   "source": [
    "# SHAP explainability (save-only, no display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1c4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 10: SHAP (SAVE-ONLY) WITH FEATURE LABELS + BAR + DEPENDENCE\n",
    "# ==============================\n",
    "if shap is None:\n",
    "    log(\"⚠️ SHAP not available (shap not installed). Skipping.\")\n",
    "else:\n",
    "    import joblib\n",
    "    \n",
    "    \n",
    "model_name = \"XGBoost\"\n",
    "\n",
    "FEATURE_LABELS = [\n",
    "    \"P\", \"T\", \"TSU$_{mont-Al}$\"\n",
    "    # add the rest in the correct order...\n",
    "    # e.g., \"Al/clay\", \"Si/clay\", ...\n",
    "]\n",
    "\n",
    "# Use your actual column names if you prefer:\n",
    "# FEATURE_LABELS = list(X.columns)\n",
    "\n",
    "# ---- 3) Transform X_test into model input space\n",
    "X_test_proc = pipe.named_steps[\"preprocess\"].transform(X_test)\n",
    "model = pipe.named_steps[\"model\"]\n",
    "\n",
    "# If preprocess changes feature count (e.g., one-hot), we fallback safely\n",
    "n_features_proc = X_test_proc.shape[1]\n",
    "if len(FEATURE_LABELS) != n_features_proc:\n",
    "    log(f\"⚠️ FEATURE_LABELS length ({len(FEATURE_LABELS)}) != processed features ({n_features_proc}).\")\n",
    "    # fallback: generic labels\n",
    "    FEATURE_LABELS = [f\"f{i}\" for i in range(n_features_proc)]\n",
    "\n",
    "# SHAP wants a DataFrame to show names nicely\n",
    "Xshap = pd.DataFrame(X_test_proc, columns=FEATURE_LABELS)\n",
    "\n",
    "# ---- 4) Explainer\n",
    "with capture_stdout():\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(Xshap)\n",
    "    except Exception:\n",
    "        explainer = shap.Explainer(model, Xshap)\n",
    "        shap_values = explainer(Xshap)\n",
    "\n",
    "# Standardize shap_values to an array for plotting\n",
    "sv = getattr(shap_values, \"values\", shap_values)\n",
    "\n",
    "# ---- (a) Beeswarm summary\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "with capture_stdout():\n",
    "    shap.summary_plot(sv, Xshap, show=False)\n",
    "savefig(f\"shap_summary_beeswarm_{model_name}.png\", fig=fig)\n",
    "log(f\"✅ Saved shap_summary_beeswarm_{model_name}.png\")\n",
    "\n",
    "# ---- (b) Mean(|SHAP|) bar plot\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "with capture_stdout():\n",
    "    shap.summary_plot(sv, Xshap, plot_type=\"bar\", show=False)\n",
    "savefig(f\"shap_mean_bar_{model_name}.png\", fig=fig)\n",
    "log(f\"✅ Saved shap_mean_bar_{model_name}.png\")\n",
    "\n",
    "# ---- (c) Dependence plots like your example\n",
    "# Choose feature pairs here (edit names to match your labels)\n",
    "dependence_pairs = [\n",
    "    (\"P\", \"T\"),\n",
    "    (\"T\", \"P\"),\n",
    "    (\"TSU$_{mont-Al}$\", \"P\"),\n",
    "]\n",
    "\n",
    "for main_feat, interaction_feat in dependence_pairs:\n",
    "    if main_feat in Xshap.columns and interaction_feat in Xshap.columns:\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        ax = plt.gca()\n",
    "        with capture_stdout():\n",
    "            shap.dependence_plot(\n",
    "                main_feat, sv, Xshap,\n",
    "                interaction_index=interaction_feat,\n",
    "                show=False, ax=ax\n",
    "            )\n",
    "        # title like your figure panels\n",
    "        ax.set_title(f\"{main_feat} against {interaction_feat}\")\n",
    "        savefig(f\"shap_dependence_{main_feat}_vs_{interaction_feat}_{model_name}.png\", fig=fig)\n",
    "        log(f\"✅ Saved dependence: {main_feat} vs {interaction_feat}\")\n",
    "    else:\n",
    "        log(f\"⚠️ Skipped dependence plot: {main_feat} or {interaction_feat} not in labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6574159",
   "metadata": {},
   "source": [
    "# Final “run report” saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13fb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CELL 11: FINAL REPORT\n",
    "# ==============================\n",
    "report_lines = []\n",
    "report_lines.append(\"RUN COMPLETE\\n\")\n",
    "report_lines.append(f\"Data file: {DATA_FILE}\\n\")\n",
    "report_lines.append(f\"Rows/Cols: {df.shape}\\n\")\n",
    "report_lines.append(f\"Target: {TARGET_COL}\\n\")\n",
    "report_lines.append(f\"Numeric cols: {len(num_cols)} | Categorical cols: {len(cat_cols)}\\n\")\n",
    "\n",
    "metrics_path = OUTPUT_DIR / \"metrics_all_models.csv\"\n",
    "if metrics_path.exists():\n",
    "    report_lines.append(f\"Metrics: {metrics_path.name}\\n\")\n",
    "\n",
    "if (OUTPUT_DIR / \"BEST_pipeline.joblib\").exists():\n",
    "    report_lines.append(\"Best model saved: BEST_pipeline.joblib\\n\")\n",
    "    if (OUTPUT_DIR / \"best_model_metrics.json\").exists():\n",
    "        report_lines.append(\"Best model metrics: best_model_metrics.json\\n\")\n",
    "\n",
    "save_text(\"RUN_REPORT.txt\", \"\".join(report_lines))\n",
    "log(\"✅ Saved RUN_REPORT.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
